{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This technical report documents the process and findings of a project aimed at classifying cardiovascular diseases using machine learning techniques. The project begins with exploratory data analysis (EDA) of a dataset obtained from Kaggle, focusing on understanding key attributes and identifying data inaccuracies. Subsequently, feature engineering is conducted to enhance the dataset's predictive power, followed by model design and evaluation. The report aims to provide insights into the data, methodology, and outcomes of the classification task, facilitating informed decision-making for future studies or applications in the field of healthcare analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exploratory Data Analysis (EDA)\n",
    "Conducted EDA using pandas, matplotlib, and seaborn to answer specific questions regarding the dataset, including:\n",
    "Distribution of positive and negative cases for cardiovascular disease.\n",
    "Distribution of cholesterol levels and age.\n",
    "Proportion of men and women with Cardio Disease\n",
    "Proportion of smokers, weight distribution, and gender distribution.\n",
    "Summary of results presented in Markdown boxes within a notebook format and graph's.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_cardio = pd.read_csv('../Lab/data/cardio_train.csv', sep=';', index_col='id')\n",
    "# Convert age from days to years in the dataset and round the age\n",
    "df_cardio['age'] = round(df_cardio['age'] / 365)\n",
    "# Rename the columns\n",
    "df_cardio.rename(columns={'ap_hi': 'systolic', 'ap_lo': 'diastolic', 'cardio': 'cardio_disease', 'gluc': 'glucose_level', 'alco': 'alcohol_intake', 'active': 'physical_activity', 'smoke': 'smoking', 'cholesterol': 'cholesterol_level'}, inplace=True)\n",
    "# Drop the id column\n",
    "#df_cardio.drop('id', axis=1, inplace=True)\n",
    "df_cardio.head()\n",
    "df_copy = df_cardio.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Age Conversion: The age column, originally containing values in days, was converted to represent years. \n",
    "\n",
    "- Column Renaming: Several column names were modified to improve clarity and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardi_disease_positiv = df_cardio[\"cardio_disease\"].sum() # Count the number of 1's\n",
    "cardio_disease_negativ = len(df_cardio) - cardi_disease_positiv # Count the number of 0's\n",
    "print(f\"Antal positiva: {cardi_disease_positiv}\")\n",
    "print(f\"Antal negativa: {cardio_disease_negativ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of positive and negative cases for cardiovascular disease:\n",
    "Quantity positiv: 34979\n",
    "Quantity negativ: 35021\n",
    "\n",
    "Distribution of cholesterol levels:\n",
    "Normal: 74.84%\n",
    "Above normal: 13.64%\n",
    "Well above normal: 11.52%\n",
    "\n",
    "Proportion of smokers:\n",
    "percentage smokers: 8.81%\n",
    "\n",
    "Proportion of men and women with Cardio Disease:\n",
    "Percentage of Men with Cardio Disease: 50.5 % \n",
    "Percentage of Women with Cardio Disease: 49.7 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many in the cholesterol_level column are 1, 2 or 3 normal, above normal and well above normal count the percentage\n",
    "cholesterol_level = df_cardio['cholesterol_level'].value_counts()\n",
    "cholesterol_level_percentage = df_cardio['cholesterol_level'].value_counts(normalize=True) * 100\n",
    "# print the percentage for the cholesterol_level and normal, above normal and well above normal\n",
    "print(f\"Normal: {cholesterol_level_percentage[1]:.2f}%\")\n",
    "print(f\"Above normal: {cholesterol_level_percentage[2]:.2f}%\")\n",
    "print(f\"Well above normal: {cholesterol_level_percentage[3]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rökare = df_cardio[df_cardio[\"smoking\"] == 1].shape[0] # Count the number of smokers\n",
    "total = df_cardio.shape[0] # Count the total number of patients\n",
    "print(f\"Percentage smokers: {rökare / total:.2%}\") # Print the percentage of smokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histoplot of age distribution\n",
    "plt.figure(figsize=(16, 2)) # Set the size of the plot\n",
    "sns.histplot(data=df_cardio, x=\"age\", element=\"step\") # Create a histogram of the age column\n",
    "plt.xlabel(\"Age\") # Set the x-axis label\n",
    "plt.ylabel(\"Quantity patient's\") # Set the y-axis label\n",
    "plt.tight_layout()\n",
    "plt.show() # Show the plot\n",
    "\n",
    "# Make a histoplot of the weight distribution\n",
    "plt.figure(figsize=(16, 2))\n",
    "sns.histplot(data=df_cardio, x=\"weight\", element=\"poly\", bins=30)\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Quantity patient's\")\n",
    "plt.show()\n",
    "\n",
    "# Make a histoplot of height distribution\n",
    "plt.figure(figsize=(16, 2))\n",
    "sns.histplot(data=df_cardio, x=\"height\", element=\"poly\", bins=40)\n",
    "plt.xlabel(\"Hight (cm)\")\n",
    "plt.ylabel(\"Quantity patient's\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of men and women with cardio disease\n",
    "# Make a copy of df_cardio\n",
    "df = df_cardio.copy()\n",
    "# Map gender values to 'Men' and 'Women'\n",
    "df['gender'] = df_cardio['gender'].map({1: 'Women', 2: 'Men'})\n",
    "# Group by gender and calculate the mean of cardio_disease (since 1 represents having disease and 0 represents not having)\n",
    "percentage_by_gender = df.groupby('gender')['cardio_disease'].mean() * 100\n",
    "print(percentage_by_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: \n",
    "The dataset (70,000 individuals) has a balanced distribution of positive and negative cases (cardiovascular disease). Cholesterol levels show a majority with normal levels (74.84%). Age skews towards middle age (30-65), with most patients between 50-60. Smoking prevalence is low (8.81%). Further investigation is needed for weight and height data due to potential inconsistencies and gender imbalance. Interestingly, CVD prevalence is nearly equal between men and women despite the imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Model design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The analysis reveales no significant correlations between any of the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(df_cardio.corr(), annot=True, fmt=\".1f\", vmin=-1, vmax=1, cmap='coolwarm')\n",
    "# Add title\n",
    "plt.title('Correlation Matrix')\n",
    "# Save the plot so i can display togheer with the other plots\n",
    "plt.savefig('correlation_matrix.png')\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Values in the \"weight\" column below 60 kg or exceeding 140 kg are replaced with the median weight calculated for the entire dataset (denoted by median_weight). Likewise, heights outside the 150 cm to 200 cm range in the \"height\" column were replaced with the median height (denoted by median_height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_weight = df_cardio['weight'].median() # Calculate the median weight\n",
    "median_height = df_cardio['height'].median() # Calculate the median height\n",
    "df_cardio['weight'] = np.where((df_cardio['weight'] < 60) | (df_cardio['weight'] > 140), median_weight, df_cardio['weight']) # Change the unrealistic values in weight to median values\n",
    "df_cardio['height'] = np.where((df_cardio['height'] < 150) | (df_cardio['height'] > 200), median_height, df_cardio['height']) # Change the unrealistic values in height to median values\n",
    "df_cardio['bmi'] = df_cardio['weight'] / (df_cardio['height'] / 100) ** 2 # Create a BMI column\n",
    "df_cardio['bmi'] = round(df_cardio['bmi'], 1) # Round the BMI to one decimal place\n",
    "def categorize_bmi(bmi): # Define a function to categorize BMI\n",
    "  if bmi < 18.5:  # If BMI is less than 18.5\n",
    "    return \"Underweight\" # Return the category\n",
    "  elif 18.5<= bmi <= 24.9: # If BMI is between 18.5 and 24.9\n",
    "    return \"Normal\"\n",
    "  elif 25 <= bmi <= 29.9: # If BMI is between 25 and 29.9\n",
    "    return \"Overweight\"\n",
    "  elif 30 <= bmi <= 34.9: # If BMI is between 30 and 34.9\n",
    "    return \"Obese(1)\"\n",
    "  elif 35 <= bmi <= 39.9: # If BMI is between 35 and 39.9\n",
    "    return \"Obese(2)\"\n",
    "  else:\n",
    "    return \"Obese(3)\"\n",
    "df_cardio['bmi_cat'] = df_cardio['bmi'].apply(categorize_bmi)# Apply the function to 'bmi' column with operation apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created a new feature for Body Mass Index (BMI) based on weight and height\n",
    "- The BMI feature did not exhibit strong correlations with other features\n",
    "- Categorized BMI into distinct classes: Underweight, Normal, Overweight, and different levels of Obesity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blood Pressure Outlier Handling:\n",
    "\n",
    "Values considered physiologically implausible (systolic blood pressure  < 60 mmHg or > 220 mmHg, diastolic blood pressure < 40 mmHg or > 120 mmHg) were replaced with the median value of the respective column (\"systolic\" or \"diastolic\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the blood pressure values and fill the outliers with median\n",
    "df_cardio['systolic'] = df_cardio['systolic'].apply(lambda x: df_cardio['systolic'].median() if x > 220 or x < 60 else x)\n",
    "df_cardio['diastolic'] = df_cardio['diastolic'].apply(lambda x: df_cardio['diastolic'].median() if x > 120 or x < 40 else x)\n",
    "\n",
    "# Create a categorical variable for blood pressure\n",
    "def categorize_blood_pressure(systolic, diastolic):\n",
    "  if systolic < 120 and diastolic < 80:\n",
    "    return \"Healthy\"\n",
    "  elif 120 <= systolic <= 130 and diastolic < 80:\n",
    "    return \"Elevated\"\n",
    "  elif 130 <= systolic <= 139 or 80 <= diastolic <= 89:\n",
    "    return \"Stage 1 Hypertension\"\n",
    "  elif systolic >= 140 or diastolic >= 90:\n",
    "    return \"Stage 2 Hypertension\"\n",
    "  elif systolic >180 or diastolic > 120:\n",
    "    return \"Hypertension Crisis\"\n",
    "  else: \n",
    "    return \"Invalid\"\n",
    "df_cardio['blood_pressure'] = df_cardio.apply(lambda x: categorize_blood_pressure(x['systolic'], x['diastolic']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorized blood pressure into relevant categories according to medical guidelines.\n",
    "\n",
    "- The __blood_presure__ feature did not exhibit strong correlations with other features after creating dummy variables for the categories and creating a correlation matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Create two data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio_copy1 = df_cardio.copy() # Create a copy of the dataset for the next task\n",
    "df_cardio_copy2 = df_cardio.copy() # Create a copy of the dataset for the next task\n",
    "\n",
    "# Remove the 'bmi' p hi, ap lo, height, weight from the first dataset\n",
    "df_cardio_copy1.drop(['bmi', 'systolic', 'diastolic', 'height', 'weight'], axis=1, inplace=True)\n",
    "# Do a hot encoding for the first dataset for one-hot encoding p ̊a BMI_cat, blod_pressure and gender\n",
    "df_cardio_copy1 = pd.get_dummies(df_cardio_copy1, columns=['bmi_cat', 'blood_pressure', 'gender'], drop_first=False)\n",
    "df_cardio_copy1.head()\n",
    "# Remove the 'bmi_cat', 'blood_pressure from the second dataset\n",
    "df_cardio_copy2.drop(['bmi_cat', 'blood_pressure', 'height', 'weight'], axis=1, inplace=True)\n",
    "# Do a hot enconding for the second dataset on gender\n",
    "df_cardio_copy2 = pd.get_dummies(df_cardio_copy2, columns=['gender'], drop_first=False)\n",
    "df_cardio_copy2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of three machine learning algorithms: Random Forest, Logistic Regression, and K-Nearest Neighbors (KNN).\n",
    "Utilized GridSearchCV for hyperparameter tuning and cross-validation.\n",
    "Split the dataset into training and testing sets for each algorithm and evaluated model performance using accuracy and other relevant metrics.\n",
    "Analyzed the best-performing model for each algorithm and dataset combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Split the data into features and target in the first dataset\n",
    "X1, y1 = df_cardio_copy1.drop('cardio_disease', axis=1), df_cardio_copy1['cardio_disease']\n",
    "# Split the data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3, random_state=1)\n",
    "\n",
    "# Split the data into features and target in the second dataset\n",
    "X2, y2 = df_cardio_copy2.drop('cardio_disease', axis=1), df_cardio_copy2['cardio_disease'] # Split the data into features and target\n",
    "# Split the data into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a pipeline with a scaler and random forest classifier\n",
    "pipe = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=1))\n",
    "# create a parameter grid: map the parameter names to the values that should be searched for the pipeline\n",
    "param_grid = {\n",
    "    'randomforestclassifier__n_estimators': [50, 100, 200], # Number of trees in the forest\n",
    "    'randomforestclassifier__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None] # Maximum depth of the tree\n",
    "}\n",
    "# Create a grid search object\n",
    "grid_knn = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentin/.local/share/virtualenvs/ML_Valentin-pyoAUsqH/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/valentin/.local/share/virtualenvs/ML_Valentin-pyoAUsqH/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit Grid Search on the first dataset\n",
    "grid_knn.fit(X_train1, y_train1)\n",
    "\n",
    "# Evaluate the best model on the testing data\n",
    "best_model1 = grid_knn.best_estimator_\n",
    "y_pred1 = best_model1.predict(X_test1)\n",
    "\n",
    "# Calculate accuracy and other relevant metrics\n",
    "accuracy1 = accuracy_score(y_test1, y_pred1)\n",
    "report1 = classification_report(y_test1, y_pred1)\n",
    "\n",
    "# Hyperparameter Analysis\n",
    "best_params1 = grid_knn.best_params_\n",
    "\n",
    "# Data Collection for Presentation\n",
    "# Collect relevant data for presentation\n",
    "\n",
    "# Repeat the above steps for the second dataset\n",
    "grid_knn.fit(X_train2, y_train2)\n",
    "best_model2 = grid_knn.best_estimator_\n",
    "y_pred2 = best_model2.predict(X_test2)\n",
    "accuracy2 = accuracy_score(y_test2, y_pred2)\n",
    "report2 = classification_report(y_test2, y_pred2)\n",
    "best_params2 = grid_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valentin/.local/share/virtualenvs/ML_Valentin-pyoAUsqH/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid_logic = {\n",
    "    'logisticregression__C': [0.01, 0.1, 1, 10, 100],  # Explore different regularization strengths\n",
    "    'logisticregression__penalty': ['l2'],  # Limit search to L2 penalty (can be modified)\n",
    "    'logisticregression__solver': ['lbfgs', 'saga' ],  # Consider including other solvers\n",
    "    'logisticregression__max_iter': [10000],  # Increase max_iter for convergence\n",
    "}\n",
    "# Create a pipeline for Logistic Regression with preprocessing\n",
    "pipe_logic = make_pipeline(StandardScaler(), LogisticRegression(random_state=1))\n",
    "# Create the GridSearchCV object for hyperparameter tuning\n",
    "grid_search_logic = GridSearchCV(pipe_logic, param_grid_logic, cv=10, n_jobs=-1, scoring='accuracy')\n",
    "# Fit Grid Search on the first dataset\n",
    "grid_search_logic.fit(X_train1, y_train1)\n",
    "\n",
    "# Evaluate the best logistic regression model on the testing data\n",
    "best_model_logic1 = grid_search_logic.best_estimator_\n",
    "y_pred_logic1 = best_model_logic1.predict(X_test1)\n",
    "\n",
    "# Calculate accuracy and other relevant metrics\n",
    "accuracy_logic1 = accuracy_score(y_test1, y_pred_logic1)\n",
    "report_logic1 = classification_report(y_test1, y_pred_logic1)\n",
    "\n",
    "# Hyperparameter Analysis\n",
    "best_params_logic1 = grid_search_logic.best_params_\n",
    "\n",
    "# Data Collection for Presentation\n",
    "# Collect relevant data for presentation\n",
    "\n",
    "# Repeat the above steps for the second dataset\n",
    "grid_search_logic.fit(X_train2, y_train2)\n",
    "best_model_logic2 = grid_search_logic.best_estimator_\n",
    "y_pred_logic2 = best_model_logic2.predict(X_test2)\n",
    "accuracy_logic2 = accuracy_score(y_test2, y_pred_logic2)\n",
    "report_logic2 = classification_report(y_test2, y_pred_logic2)\n",
    "best_params_logic2 = grid_search_logic.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create a pipeline for the KNN model\n",
    "pipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "# Define the hyperparameter grid for the KNN model\n",
    "param_grid_knn = {\n",
    "    'kneighborsclassifier__n_neighbors': range(1, 21),\n",
    "    'kneighborsclassifier__weights': ['uniform', 'distance'],\n",
    "    'kneighborsclassifier__p': [1, 2], \n",
    "}\n",
    "# Create the GridSearchCV object\n",
    "grid_search_knn = GridSearchCV(pipe_knn, param_grid_knn, cv=5, n_jobs=-1)\n",
    "# Fit Grid Search on the first dataset\n",
    "grid_search_knn.fit(X_train1, y_train1)\n",
    "\n",
    "# Evaluate the best KNN model on the testing data\n",
    "best_model_knn1 = grid_search_knn.best_estimator_\n",
    "y_pred_knn1 = best_model_knn1.predict(X_test1)\n",
    "\n",
    "# Calculate accuracy and other relevant metrics\n",
    "accuracy_knn1 = accuracy_score(y_test1, y_pred_knn1)\n",
    "report_knn1 = classification_report(y_test1, y_pred_knn1)\n",
    "\n",
    "# Hyperparameter Analysis\n",
    "best_params_knn1 = grid_search_knn.best_params_\n",
    "\n",
    "# Data Collection for Presentation\n",
    "# Collect relevant data for presentation\n",
    "\n",
    "# Repeat the above steps for the second dataset\n",
    "grid_search_knn.fit(X_train2, y_train2)\n",
    "best_model_knn2 = grid_search_knn.best_estimator_\n",
    "y_pred_knn2 = best_model_knn2.predict(X_test2)\n",
    "accuracy_knn2 = accuracy_score(y_test2, y_pred_knn2)\n",
    "report_knn2 = classification_report(y_test2, y_pred_knn2)\n",
    "best_params_knn2 = grid_search_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the results\n",
    "results_list = []\n",
    "\n",
    "# Define a function to append results to the list\n",
    "def append_results(model_name, dataset_name, accuracy, best_params, classification_report):\n",
    "    results_list.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Best Parameters': best_params,\n",
    "        'Classification Report': classification_report\n",
    "    })\n",
    "\n",
    "# Append results for the RandomForestClassifier\n",
    "append_results('Random Forest', 'Dataset 1', accuracy1, best_params1, report1)\n",
    "append_results('Random Forest', 'Dataset 2', accuracy2, best_params2, report2)\n",
    "\n",
    "# Append results for the Logistic Regression model\n",
    "append_results('Logistic Regression', 'Dataset 1', accuracy_logic1, best_params_logic1, report_logic1)\n",
    "append_results('Logistic Regression', 'Dataset 2', accuracy_logic2, best_params_logic2, report_logic2)\n",
    "\n",
    "# Append results for the KNN model\n",
    "append_results('KNN', 'Dataset 1', accuracy_knn1, best_params_knn1, report_knn1)\n",
    "append_results('KNN', 'Dataset 2', accuracy_knn2, best_params_knn2, report_knn2)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis, while Logistic Regression may offer faster runtime compared to Random Forest, both models demonstrated competitive performance. Given the balance between speed and performance, Logistic Regression could be favored for tasks prioritizing computational efficiency, while Random Forest remains a strong choice for maximizing predictive accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "id                                                                         \n",
       "0   50.0       2     168    62.0    110     80            1     1      0   \n",
       "1   55.0       1     156    85.0    140     90            3     1      0   \n",
       "2   52.0       1     165    64.0    130     70            3     1      0   \n",
       "3   48.0       2     169    82.0    150    100            1     1      0   \n",
       "4   48.0       1     156    56.0    100     60            1     1      0   \n",
       "\n",
       "    alco  active  cardio  \n",
       "id                        \n",
       "0      0       1       0  \n",
       "1      0       1       1  \n",
       "2      0       0       1  \n",
       "3      0       1       1  \n",
       "4      0       0       0  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Load the data\n",
    "df = pd.read_csv('../Lab/data/cardio_train.csv', sep=';', index_col='id')\n",
    "# Convert age from days to years in the dataset and round the age\n",
    "df['age'] = round(df['age'] / 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_model.pkl']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into features and target\n",
    "\n",
    "X, y = df.drop('cardio', axis=1), df['cardio']\n",
    "\n",
    "# Randomly select 100 samples\n",
    "X_test, _, y_test, _ = train_test_split(X, y, test_size=100, random_state=42)\n",
    "\n",
    "# Export 100 samples to a CSV file\n",
    "X_test.to_csv('test_samples.csv', index=False)\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model_logic2.fit(X, y)  # 'best_model' represents your best-performing model\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(best_model_logic2, 'trained_model.pkl', compress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test samples and the trained model\n",
    "test_samples = pd.read_csv('test_samples.csv')\n",
    "model = joblib.load('trained_model.pkl')\n",
    "\n",
    "# Make predictions on the test samples\n",
    "predictions = model.predict(test_samples)\n",
    "\n",
    "# Get probabilities for each class\n",
    "probabilities = model.predict_proba(test_samples)\n",
    "prob_class_0 = probabilities[:, 0]\n",
    "prob_class_1 = probabilities[:, 1]\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    'probability class 0': prob_class_0,\n",
    "    'probability class 1': prob_class_1,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "# Export predictions to CSV\n",
    "prediction_df.to_csv('prediction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Valentin-pyoAUsqH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
