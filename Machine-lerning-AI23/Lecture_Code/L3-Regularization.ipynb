{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture notes - Regularized linear models\n",
    "---\n",
    "\n",
    "This is the lecture note for **regularized linear models**\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that this lecture note gives a brief introduction to regularized linear models. I encourage you to read further about regularized linear models. </p>\n",
    "\n",
    "Read more:\n",
    "\n",
    "- [Regularized linear models medium](https://medium.com/analytics-vidhya/regularized-linear-models-in-machine-learning-d2a01a26a46)\n",
    "- [Ridge regression wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "- [Tikhonov regularization wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "- [Lasso regression wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
    "- [Korsvalidering](https://sv.wikipedia.org/wiki/Korsvalidering)\n",
    "- [Cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [Scoring parameter sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [ISLP pp 240-253](https://www.statlearning.com/)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 19), (66, 19), (134,), (66,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Advertising.csv', index_col=0)\n",
    "\n",
    "X, y = df.drop('Sales', axis=1), df['Sales'] # X is a DataFrame, y is a Series\n",
    "# Detta är en multiple polynomial regression\n",
    "# termerna är x1^3, x2^3, x3^3, x1^3*x2^3*x3^3, x1^2, x2^2, x3^2, x1^2*x2^2*x3^2, x1*x2, x1*x3, ...osv. Totalt 19 termer/features\n",
    "# övning 3 (e02) har en uppgift för att hitta graden neda. Denan är vald konservativt eftersom antalet features ökar snabbt med graden\n",
    "# för polynomet och vi vill undvika overfitting\n",
    "\n",
    "model_poly = PolynomialFeatures(degree=3, include_bias=False) # include_bias=False betyder att vi inte vill ha en konstant term ,degree=3 betyder att vi vill ha polynom av grad 3\n",
    "poly_features = model_poly.fit_transform(X) # skapar nya features som är polynom av de gamla features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.33, random_state=42)\n",
    "# Vi delar upp datan i en träningsmängd och en testmängd\n",
    "# train_test_split returnerar fyra värden, X_train, X_test, y_train, y_test där X_train och X_test är träningsmängden och testmängden för features och y_train och y_test är träningsmängden och testmängden för target\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Feature standardization \n",
    "\n",
    "Remove sample mean and divide by sample standard deviation \n",
    "\n",
    "$X' = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "LASSO, Ridge and Elasticnet regression that we'll use later require that the data is scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X_train mean: -0.00 and std: 1.00\n",
      "Scaled X_test mean: -0.12 and std: 1.12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train) # skalar träningsmängden\n",
    "scaled_X_test = scaler.transform(X_test) # skalar testmängden\n",
    "\n",
    "print(f'Scaled X_train mean: {scaled_X_train.mean():.2f} and std: {scaled_X_train.std():.2f}')\n",
    "print(f'Scaled X_test mean: {scaled_X_test.mean():.2f} and std: {scaled_X_test.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization techniques \n",
    "\n",
    "Problem with overfitting was discussed in previous lecture. When a model is to complex, data is noisy and too small the model picks upp pattern in the noise. The output of a lin-reg is the weighted sum $y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n$, where the weights $\\theta_i$ represents the importance of the $ith$ feature. We want to constraint the weight associated with noise, through **regularization**. We do this by **adding a regularization term** to the cost function used in training model. **Note** that the cost function for evaluation now will differ from training.\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> most regularization model requires scaling of data </p>\n",
    "\n",
    "---\n",
    "### Ridge regression \n",
    "Also called Tikhonov regularization or $\\ell_2$ regularization.\n",
    "\n",
    "$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the ridge parameter or the penalty term, which reduces variance by increasing bias. Observe that the sum starts from 1, so the bias term $\\theta_0$ is not affected by $\\lambda$. Therefore by the larger the $\\lambda$ the more $\\theta_i, i = {1,2,\\ldots}$ causes higher error. As variance is decreasing and bias increasing, the model fits worse to the training datas noise and generalizes better.\n",
    "\n",
    "From the closed form OLS solution to ridge regression, we see that $\\lambda = 0$ gives us the normal equation for linear regression: \n",
    "\n",
    "$\\hat{\\vec{\\theta}} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6109310380379472, 0.4845959994544078)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae\n",
    "\n",
    "def ridge_regression(X, penalty=0):\n",
    "    # alpha = 0 betyder att vi inte har någon regularisering ger oss en vanlig linjär regression\n",
    "    # note that the default alpha is 1.0\n",
    "    # note that alpha is the same as lambda in teory i.e the penalty term in the cost function, sklearn has chosen alpha to generalize their API\n",
    "    model_ridge = Ridge(alpha=penalty)\n",
    "    model_ridge.fit(scaled_X_train, y_train)\n",
    "    y_pred_train = model_ridge.predict(X)\n",
    "    return y_pred_train\n",
    "\n",
    "y_pred_train = ridge_regression(scaled_X_test, 0.2)\n",
    "MSE = mse(y_test, y_pred_train)\n",
    "RMSE = np.sqrt(MSE) # RMSE is more interpretable since it's in the same units as the target\n",
    "\n",
    "RMSE, mae(y_test, y_pred_train) # Mean Absolute Error for the test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5148267621786599, 0.37485164412178346)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with linear regression -> Rmse very similar\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_lin = LinearRegression() # skapar en instans av en linjär regressionsmodell\n",
    "model_lin.fit(scaled_X_train, y_train) # tränar modellen på träningsmängden\n",
    "y_pred_train_lin = model_lin.predict(scaled_X_test) # gör förutsägelser på testmängden\n",
    "np.sqrt(mse(y_test, y_pred_train_lin)), mae(y_test, y_pred_train_lin) # Mean Absolute Error for the test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.89480144  0.42062367  0.         -0.          3.55216501  0.\n",
      "  0.          0.01110965  0.         -0.42677394 -0.         -0.\n",
      "  0.          0.         -0.          0.          0.06706906  0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model_lasso = Lasso(alpha=0.1) # alpha=0.1 betyder att vi har en liten regularisering\n",
    "model_lasso.fit(scaled_X_train, y_train) # tränar modellen på träningsmängden\n",
    "y_pred_train_lasso = model_lasso.predict(scaled_X_test) # gör förutsägelser på testmängden\n",
    "np.sqrt(mse(y_test, y_pred_train_lasso)), mae(y_test, y_pred_train_lasso) # Mean Absolute Error for the test set predictions\n",
    "print(model_lasso.coef_) # printar ut koefficienterna för modellen\n",
    "\n",
    "# Lasso ger oss en sparse model, dvs den ger oss en modell med färre features än vad vi började med\n",
    "# Detta är användbart om vi har många features och vill ha en enklare modell\n",
    "# Så här kan vi använda Lasso för att göra feature selection\n",
    "# Vi ser att Lasso har satt koefficienten för vissa features till 0, dvs den har tagit bort dem från modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold-validation \n",
    "\n",
    "One strategi to choose the **best hyperparameter alpha** is to take the training part of the data and:\n",
    "\n",
    " 1. shuffle dataset randomly \n",
    " 2. split into k-groups \n",
    " 3. for each group -> take one test, the rest training -> fit the model -> predict on test -> get evaluation metric\n",
    " 4. take the mean of evaluation metrics\n",
    " 5. choose the parameters and train on the entire training dataset\n",
    "\n",
    "Repeat the process for each alpha, to see which yields lowest RMSE k-fold cross validation:\n",
    "\n",
    "- good for smaller datasets \n",
    "- fair evaluation, as mean of evaluation metric for all k-groups is calculated \n",
    "- expensive to compute as it requires k+1 times of training \n",
    "\n",
    "___\n",
    "\n",
    "### Ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV # RidgeCV är en variant av Ridge som har inbyggd cross-validation för att hitta det bästa värdet på alpha\n",
    "from sklearn.metrics import make_scorer # make_scorer används för att skapa en scorer som vi kan använda i cross-validation\n",
    "#from sklearn.metrics import SCORERS # SCORERS är en dictionary som innehåller alla scorer som finns i sklearn\n",
    "\n",
    "#SCORERS.keys() # printar ut alla scorer som finns i sklearn\n",
    "# Vi kan använda make_scorer för att skapa en scorer som vi kan använda i cross-validation\n",
    "# Vi kan använda denna scorer för att hitta det bästa värdet på alpha i RidgeCV\n",
    "# negative because sklearn uses the convention that the higher the score, the better the model\n",
    "\n",
    "model_ridge_cv = RidgeCV(alphas=[0.0001, .001, .01, .1, 5, 10], scoring=make_scorer(mse, greater_is_better=False))\n",
    "model_ridge_cv.fit(scaled_X_train, y_train)\n",
    "model_ridge_cv.alpha_ # best alpha\n",
    "print(model_ridge_cv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5635899169610441, 0.4343075766545298)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best alpha is 0.1\n",
    "# it seams that linear regression has outperformed ridge regression in this case\n",
    "# however, this is not always the case, ridge regression is more robust to multicollinearity and overfitting\n",
    "# it's also more interpretable since it shrinks the coefficients towards zero\n",
    "# it could olso depend on the distribution of the train test data so using 0.1 is more robust here\n",
    "\n",
    "y_pred_train_ridge_cv = model_ridge_cv.predict(scaled_X_test)\n",
    "RMSE = np.sqrt(mse(y_test, y_pred_train_ridge_cv))\n",
    "RMSE, mae(y_test, y_pred_train_ridge_cv) # Mean Absolute Error for the test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.84681185,  0.52142086,  0.71689997, -6.17948738,  3.75034058,\n",
       "       -1.36283352, -0.08571128,  0.08322815, -0.34893776,  2.16952446,\n",
       "       -0.47840838,  0.68527348,  0.63080799, -0.5950065 ,  0.61661989,\n",
       "       -0.31335495,  0.36499629,  0.03328145, -0.13652471])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridge_cv.coef_ # printar ut koefficienterna för modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.004968802520343366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5785146895301982, 0.46291883026933045)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# it is trying 100 alphas along the regularization path epsilon\n",
    "model_lasso_cv = LassoCV(eps=0.001, n_alphas=100, max_iter=10000, cv=5) # cv=5 betyder att vi använder 5-fold cross-validation\n",
    "model_lasso_cv.fit(scaled_X_train, y_train) # tränar modellen på träningsmängden\n",
    "print(f'alpha = {model_lasso_cv.alpha_}') # best alpha\n",
    "\n",
    "y_pred_train_lasso_cv = model_lasso_cv.predict(scaled_X_test) # gör förutsägelser på testmängden\n",
    "np.sqrt(mse(y_test, y_pred_train_lasso_cv)), mae(y_test, y_pred_train_lasso_cv) # Mean Absolute Error for the test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.19612354,  0.43037087,  0.29876351, -4.80417579,  3.46665205,\n",
       "       -0.40507212,  0.        ,  0.        ,  0.        ,  1.35260206,\n",
       "       -0.        ,  0.        ,  0.14879719, -0.        ,  0.        ,\n",
       "        0.        ,  0.09649665,  0.        ,  0.04353956])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that LassoCV has set some coefficients to zero\n",
    "# this means that LassoCV has performed feature selection\n",
    "# it has removed some features from the model\n",
    "# this is useful if we have many features and want a simpler model\n",
    "\n",
    "model_lasso_cv.coef_ # printar ut koefficienterna för modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net \n",
    "\n",
    "Elastic net is a combination of both **Ridge L2 regularization** and **Lasso regularization L1**. The cost function to minimize elastic net is:\n",
    "\n",
    "$$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda\\left(\\alpha\\sum_{i=1}^n |\\theta_i| + \\frac{1-\\alpha}{2}\\sum_{i=1}^n \\theta_i^2\\right)$$\n",
    "\n",
    ", where $\\alpha$ here determines the ratio for $\\ell_1$ or $\\ell_2$ regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 ratio = 1.0\n",
      "alpha = 0.004968802520343366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# note that alpha is the same as lambda in teory i.e the penalty term in the cost function, sklearn has chosen alpha to generalize their API\n",
    "# l1_ratio is alpha in the cost function, i.e the ratio between the l1 and l2 penalty\n",
    "\n",
    "model_elastic_net_cv = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps=0.001, n_alphas=100, max_iter=10000)\n",
    "model_elastic_net_cv.fit(scaled_X_train, y_train)\n",
    "print(f'L1 ratio = {model_elastic_net_cv.l1_ratio_}') # best l1 ratio (alpha) remove ridge and pick lasso entirely\n",
    "print(f'alpha = {model_elastic_net_cv.alpha_}') # best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5785146895301982, 0.46291883026933045)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train_elastic_net_cv = model_elastic_net_cv.predict(scaled_X_test)\n",
    "np.sqrt(mse(y_test, y_pred_train_elastic_net_cv)), mae(y_test, y_pred_train_elastic_net_cv) # Mean Absolute Error for the test set predictions\n",
    "# note that result is same as lasso regression because l1_ratio is 1 so it is expected to be same as lasso regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Valentin-pyoAUsqH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
